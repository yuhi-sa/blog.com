---
title: "[Python]Unscented Transformation"
date: 2021-01-25T15:17:23+09:00
draft: false
tags: ["MachineLearning"] 
---
<!--more-->
The U-transform is a method for estimating $\bar{y}$ and $P_y$ of a random variable $y$ transformed by a nonlinear transformation $y=f(x)$ of $x$ when the mean $\bar{x}$ and variance-covariance matrix $P_x$ of the random variable $x$ following a standard normal distribution are known.

First of all, it is possible to compute the following from the Monte Carlo point of view, but this requires a large value of N to achieve good accuracy, which poses a practical problem.

$$
\bar{y}\simeq\frac{1}{N}\sum_{i=1}^Nf(x_i)
$$

$$
P_y \simeq \frac{1}{N}\sum_{i=1}^N(f(x_i)-\bar{y})(f(x_i)-\bar{y})^T
$$

The U-transform is a method for estimating the statistical properties of a random variable after a transformation, using as few sample points as possible so that the good points of the Montecarlo method can be used without linear approximation.

First, determine the value to be sampled from the random variable $x$ (sigma point), and then perform a nonlinear transformation of the sigma point.
Then, from the transformed values, obtain $\bar{y}$ and $P_y$ for $y$.

Reference1：[UKF （Unscented Kalman Filter）っ て何 ？](https://www.jstage.jst.go.jp/article/isciesci/50/7/50_KJ00004329717/_pdf)

```python
import matplotlib.pyplot as plt
import numpy as np
import random
import math
import scipy.linalg
```
Use $X=(X_1,X_2)$ for the input.  

mean vector

$$
\mu=(E[X_1],E[X_2])=(\bar{x}_1,\bar{x}_2)
$$
dispersion covariance vector

$$
P_x = [
    \begin{array}{cc}
      var[X_1] & cov[X_1,X_2] \\
      cov[X_2,X_1] & var[X_2]
    \end{array}
    ]
    = [
    \begin{array}{cc}
      \sigma_1^2 & \sigma_1\sigma_2 \\
      \sigma_1\sigma_2 & \sigma_2^2
    \end{array}
    ]
$$

Consider using a standard normal distribution with mean 0 variance 1 and mean 1 variance 2 for $X_1,X_2$ respectively. 

$$
\bar{x}=[0, 1]
$$

$$
P_x = [
    \begin{array}{cc}
      1 & 2 \\
      2 & 4
    \end{array}
    ]
$$

This is used as the input. The output is 1-dimensional, and we consider the nonlinear transformation $f(x)=x[0]*x[1]$.

```python
# Number of dimensions of x
n = 2
# Number of dimensions of y
m = 1

# Mean and Variance of x
x_mean = np.array([0, 1])
x_P = np.array([[1,2],[2,4]])
print("Mean",x_mean)
print("Variance",x_P)

# Nonlinear transformation of x
def f(x):
  return [x[0]*x[1]]
```

### 2. Sigma Point Calculation
Sigma points are carefully chosen representative points that accurately capture the mean and covariance of the input random variable $x$. These points are calculated using the following formulas:

$ \sigma_0 = \bar{x} \tag{1} $
$ \sigma_i = \bar{x} + (\sqrt{(n+\lambda)P_x})_i \quad \text{for } i=1, \dots, n \tag{2} $
$ \sigma_i = \bar{x} - (\sqrt{(n+\lambda)P_x})_i \quad \text{for } i=n+1, \dots, 2n \tag{3} $

Here, $ (\sqrt{(n+\lambda)P_x})_i $ represents the $i$-th column of the matrix square root (e.g., obtained via Cholesky decomposition) of $(n+\lambda)P_x$.

The parameter $\lambda$ is calculated as:
$ \lambda = \alpha^2 (n + \kappa) - n \tag{4} $

-   **$\alpha$**: A scalar parameter (typically a small positive value between 0 and 1) that determines the spread of the sigma points around the mean. A smaller $\alpha$ means the sigma points are closer to the mean.
-   **$\kappa$**: A secondary scaling parameter, usually set to 0. It further adjusts the spread of the sigma points.

Reference1: [Unscented Kalman Filter, MathWorks](https://jp.mathworks.com/help/control/ref/ukf_block.html)
Reference2: [Unscented Kalman Filter for Self-Localization MATLAB, Python Sample Program](https://myenigma.hatenablog.com/entry/20140614/1402731732)

```python
# Parameter settings
alpha = 0.5
kappa = 0.0

# Eq. 4: Calculate lambda
lambd = alpha**2 * (n + kappa) - n
print(f"lambda: {lambd}")

# Array to store sigma points (2n+1 sigma points)
sigma_points = np.zeros((n, 2 * n + 1))

# Eq. 1: The first sigma point is the mean itself
sigma_points[:, 0] = x_mean

# Calculate the matrix square root of (n+lambda) * Px (using Cholesky decomposition)
sqrt_term = scipy.linalg.cholesky((n + lambd) * x_P, lower=True)

# Eq. 2, 3: Calculate the remaining sigma points
for i in range(n):
    sigma_points[:, i + 1] = x_mean + sqrt_term[:, i]
    sigma_points[:, i + n + 1] = x_mean - sqrt_term[:, i]

print("\nGenerated sigma points:")
for i in range(2 * n + 1):
  print(f"  Sigma point {i}: {sigma_points[:, i]}")
```

### 3. Transformation and Statistical Calculation

Each calculated sigma point is transformed by the nonlinear function $f$ to obtain $y_{\sigma}$:
$ y_{\sigma, i} = f(\sigma_i) \tag{5} $

Next, the weights $w_i$ corresponding to each sigma point are calculated:
$ w_0 = \frac{\lambda}{n+\lambda} \tag{6} $
$ w_i = \frac{1}{2(n+\lambda)} \quad \text{for } i=1, \dots, 2n \tag{7} $

Finally, using these weights and the transformed sigma points $y_{\sigma}$, the mean $\bar{y}$ and covariance $P_y$ of $y$ are calculated:
$ \bar{y} \approx \sum_{i=0}^{2n} w_i y_{\sigma, i} \tag{8} $
$ P_y \approx \sum_{i=0}^{2n} w_i (y_{\sigma, i} - \bar{y})(y_{\sigma, i} - \bar{y})^T \tag{9} $

```python
# Array to store transformed sigma points
sigma_y = np.zeros((m, 2 * n + 1))
# Eq. 5: Nonlinear transformation
for i in range(2 * n + 1):
  sigma_y[:, i] = f(sigma_points[:, i])

# Array to store weights
w = np.zeros(2 * n + 1)
# Eq. 6, 7: Weight calculation
w[0] = lambd / (n + lambd)
for i in range(1, 2 * n + 1):
  w[i] = 1 / (2 * (n + lambd))

print(f"\nCalculated weights: {w}")

# Eq. 8: Calculate mean of y
y_mean = np.sum(w * sigma_y, axis=1)

# Eq. 9: Calculate covariance of y
y_P = np.zeros((m, m))
for i in range(2 * n + 1):
  diff = sigma_y[:, i] - y_mean
  y_P += w[i] * np.outer(diff, diff) # Use outer product for (m,m) matrix

print(f"\nEstimated mean of output y: {y_mean}")
print(f"Estimated covariance matrix of output y:\n{y_P}")
```
