---
title: "ニューラルネットワークを用いた教師あり学習のpythonプログラム"
date: 2021-02-15T13:00:23+09:00
draft: false
tags: ["機械学習"] 
---
<!--more-->
# ニューラルネットワークを用いた教師あり学習のpythonプログラム
## ニューラルネットワーククラスを作成
```python
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
```
[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)を継承して，ニューラルネットワーク(NN)のニューロン数，optimizer，損失関数を決める．    
損失関数によって計算される損失（loss）を小さくするように、オプティマイザがニューラルネットワークのパラメータを最適化することで学習を行います。  
- optimizerの種類：[【前編】Pytorchの様々な最適化手法(torch.optim.Optimizer)の更新過程や性能を比較検証してみた！](https://rightcode.co.jp/blog/information-technology/torch-optim-optimizer-compare-and-verify-update-process-and-performance-of-optimization-methods)  
- 損失関数の種類：[pytorch for python における損失関数 (誤差関数)](https://end0tknr.hateblo.jp/entry/20191012/1570854032)
```python
class Model(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Model, self).__init__()
        # NNの入出力
        self.fc1 = nn.Linear(input_dim, 100)
        self.fc2 = nn.Linear(100, 100)
        self.fc3 = nn.Linear(100, output_dim)
        # 学習率とパラメータの更新方法
        self.optimizer = optim.SGD(self.parameters(), lr=0.01)
        # loss関数
        self.criterion = nn.MSELoss()
```
活性化関数を定義  
- 活性化関数の種類：[活性化関数一覧 (2020)](https://qiita.com/kuroitu/items/73cd401afd463a78115a)
```python
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```
誤差逆伝播によって勾配を計算し、オプティマイザでパラメータ更新を行います。
```python
    def update(self,output, y):
        self.optimizer.zero_grad()
        loss = self.criterion(output, y)
        # 勾配の計算
        loss.backward()
        # パラメータの更新
        self.optimizer.step()
        return loss
```
## 実験
```python
# テストデータの作成
def math(x):
    return x*x + 2*x + 1

def make():
    x = np.random.rand(1000)
    y = math(x)
    return x,y

train_x, train_y = make()
test_x, test_y = make()
model = Model(1,1)

# テストデータ
gosa = 0
for i in range(1000):
    output = model(torch.tensor([[float(test_x[i])]]))
    gosa += abs((output - test_y[i]))

print("学習前：",gosa/1000)

# 訓練データを学習
for i in range(1000):
    output = model(torch.tensor([[float(train_x[i])]]))
    loss = model.update(output, torch.tensor([[float(train_y[i])]]))

# テストデータ
gosa = 0
for i in range(1000):
    output = model(torch.tensor([[float(test_x[i])]]))
    gosa += abs((output - test_y[i])[0])

print("学習後: 誤差/1000")
```
実行結果

```
学習前： tensor([[2.4048]], grad_fn=<DivBackward0>)
学習後： tensor([0.0174], grad_fn=<DivBackward0>)
```
