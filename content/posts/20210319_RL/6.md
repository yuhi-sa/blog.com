---
title: "Advantage Actor-Critic (A2C)"
date: 2021-03-22T11:30:23+09:00
draft: false
tags: ["強化学習"] 
---
<!--more-->
# Advantage Actor-Critic (A2C)

Actor-Critic法は、方策ベースの強化学習手法である方策勾配法と、価値ベースの強化学習手法であるTD学習を組み合わせたものです。Actor（方策）が行動を選択し、Critic（価値関数）がその行動を評価することで、学習を効率化します。

## アドバンテージ関数 (Advantage Function)

方策勾配法では、行動価値関数 $Q(s,a)$ を用いて方策の更新方向を決定します。しかし、行動価値関数は状態そのものの価値 $V(s)$ に大きく依存する傾向があります。例えば、ある状態の価値が非常に高い場合、その状態でのどの行動も高いQ値を持つことになります。

そこで、状態の価値 $V(s)$ を差し引いた**アドバンテージ関数** $A(s,a)$ を用いて行動を評価することを考えます。

$$ A(s,a) = Q(s,a) - V(s) $$

アドバンテージ関数は、「ある状態 $s$ において、特定の行動 $a$ を取ることが、その状態の平均的な価値 $V(s)$ と比べてどれだけ良いか（または悪いか）」を示します。これにより、行動の相対的な良さを評価できるようになります。

## 方策勾配とアドバンテージ関数

アドバンテージ関数を利用する場合の方策勾配は、以下のようになります。

$$ \nabla J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta}[\nabla \log \pi_\theta(a|s) A(s,a)] $$

この式は、方策勾配法における $Q(s,a)$ を $A(s,a)$ に置き換えた形になっています。$A(s,a)$ は、ベースラインとして機能し、勾配の分散を削減する効果があります。これにより、学習の安定性と効率が向上します。

## Advantage Actor-Critic (A2C)

アドバンテージ関数を用いたActor-Critic法は、**Advantage Actor-Critic (A2C)** と呼ばれます。

-   **Actor (アクター)**: 方策 $\pi_\theta(a|s)$ を学習し、行動を選択します。アクターは、アドバンテージ関数 $A(s,a)$ を用いて方策のパラメータ $\theta$ を更新します。
-   **Critic (クリティック)**: 状態価値関数 $V(s)$ を学習します。クリティックは、TD誤差（$r + \gamma V(s') - V(s)$）を用いて状態価値関数のパラメータを更新し、このTD誤差がアドバンテージ関数の近似としてアクターに提供されます。

A2Cは、方策勾配法の安定性を向上させ、より効率的な学習を可能にする強力なアルゴリズムです。

## 参考
-   久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔栄社 (2019)