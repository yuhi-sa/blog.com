---
title: "方策勾配法"
date: 2021-03-22T11:00:23+09:00
draft: false
tags: ["強化学習"] 
---
<!--more-->
方策勾配法（Policy Gradient Method）は、強化学習における方策ベースの手法の一つです。価値関数を学習するのではなく、**方策そのもの**をパラメータ化された関数として表現し、そのパラメータを直接最適化することで、最適な方策を見つけます。

方策は、状態 $s$ を入力として受け取り、各行動 $a$ を選択する確率 $\pi_\theta(a|s)$ を出力する関数として表現されます。ここで $\theta$ は方策のパラメータです。

## 方策の評価と最適化

方策の良さは、その方策に従って行動したときに得られる**期待報酬**（または期待累積報酬）によって評価されます。この期待報酬を最大化することが方策勾配法の目的です。

期待報酬 $J(\theta)$ は、以下のように表現できます。

$$ J(\theta) = \sum_s d^{\pi_\theta}(s) \sum_a \pi_\theta(a|s) Q^{\pi_\theta}(s,a) $$

ここで、
-   $d^{\pi_\theta}(s)$: 方策 $\pi_\theta$ の下で状態 $s$ を訪れる定常分布（または割引状態訪問頻度）です。
-   $Q^{\pi_\theta}(s,a)$: 方策 $\pi_\theta$ の下での状態 $s$ で行動 $a$ を取ったときの行動価値関数です。

この期待報酬 $J(\theta)$ を最大化するために、パラメータ $\theta$ を勾配上昇法で更新します。つまり、期待報酬の勾配 $\nabla J(\theta)$ を計算し、その方向にパラメータを少しずつ更新していきます。

## 方策勾配定理 (Policy Gradient Theorem)

方策勾配定理は、期待報酬の勾配 $\nabla J(\theta)$ を、以下のようにシンプルな形で表現できることを示しています。

$$ \nabla J(\theta) \propto \sum_s d^{\pi_\theta}(s) \sum_a \nabla \pi_\theta(a|s) Q^{\pi_\theta}(s,a) $$

さらに、対数微分トリック（Log-derivative Trick）を用いると、この勾配は期待値の形で表現できます。

$$ \nabla J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta}[\nabla \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)] $$

この式は、方策勾配を直感的に解釈する助けとなります。
-   $\nabla \log \pi_\theta(a|s)$: 行動 $a$ を選択する確率を増加させる方向（勾配）を示します。
-   $Q^{\pi_\theta}(s,a)$: その行動の「良さ」を表す重みです。

つまり、方策勾配法は、「良い行動（$Q$値が高い行動）が選択される確率を増加させ、悪い行動（$Q$値が低い行動）が選択される確率を減少させる」ように方策を更新します。

## 実装上の課題と解決策

方策勾配法の実装には、いくつかの課題があります。

-   **$Q^{\pi_\theta}(s,a)$ の推定**: 行動価値関数 $Q^{\pi_\theta}(s,a)$ は未知であるため、経験から推定する必要があります。モンテカルロ法（エピソード終了後の累積報酬）やTD学習（TD誤差）を用いて近似します。
-   **分散の削減**: 勾配の推定にモンテカルロ法を用いると、分散が大きくなり、学習が不安定になることがあります。これを解決するために、**ベースライン**（例: 状態価値関数 $V(s)$）を導入して、アドバンテージ関数 $A(s,a) = Q(s,a) - V(s)$ を用いる手法が一般的です。

方策勾配法は、連続行動空間の問題にも適用できるなど、柔軟性の高い手法であり、Actor-Critic法などのより高度なアルゴリズムの基礎となっています。

## 参考
-   久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔栄社 (2019)
