---
title: "モデルベース強化学習"
date: 2021-03-18T19:00:23+09:00
draft: false
description: "モデルベース強化学習の基礎を解説。ベルマン方程式、動的計画法による価値反復法と方策反復法、モデルフリーとの違いをわかりやすく説明します。"
tags: ["強化学習"]
---

<!--more-->

強化学習の手法は、環境のモデル（状態遷移関数や報酬関数）を利用するかどうかによって、大きく2種類に分けられます。

- **モデルベース強化学習**: 環境のモデル（状態遷移関数 $T(s'|s,a)$ や報酬関数 $R(s,s')$）を明示的に利用して行動を学習する手法です。モデルが既知でなくても、環境との相互作用を通じてモデルを推定し、それを利用することも含みます。
- **モデルフリー強化学習**: 環境のモデルを明示的に構築したり利用したりせず、エージェントが直接、経験から最適な行動を学習する手法です。

本記事では、モデルベース強化学習について解説します。

## 価値の定義

強化学習では、将来得られる報酬の合計を最大化することを目指します。この将来の報酬の合計を「価値」と呼びます。

1.  **将来の報酬の合計**: ある時刻 $t$ から将来にわたって得られる即時報酬の合計 $G_t$ は、以下のように再帰的に定義できます。
    $$ G*t = r*{t+1} + \gamma G*{t+1} $$
    ここで $r*{t+1}$ は時刻 $t+1$ で得られる即時報酬、$\gamma$ は**割引率**（$0 \le \gamma \le 1$）で、将来の報酬を現在価値に換算するための係数です。

2.  **期待値としての価値**: 環境の遷移が確率的であるため、特定の行動を取ったとしても必ずしも同じ結果が得られるわけではありません。そこで、価値は期待値として定義されます。
    ある方策 $\pi$ の下での状態 $s$ の価値関数 $V_\pi(s)$ は、状態 $s$ から開始し、方策 $\pi$ に従って行動し続けた場合に得られる累積報酬の期待値を表します。

### ベルマン方程式 (Bellman Equation)

価値関数を再帰的かつ期待値で表現したものが**ベルマン方程式**です。これは、ある状態の価値が、その状態から取りうる行動と、その行動によって遷移する次の状態の価値によって決まることを示します。

$$ V*\pi(s) = \sum_a \pi(a|s) \sum*{s'} T(s'|s,a) (R(s,a,s') + \gamma V\_\pi(s')) $$
ここで、$R(s,a,s')$ は状態 $s$ で行動 $a$ を取り、状態 $s'$ に遷移したときに得られる報酬です。

## 学習：動的計画法 (Dynamic Programming)

モデルベース強化学習では、環境のモデルが既知であるため、**動的計画法**を用いて最適な方策や価値関数を計算することができます。動的計画法では、価値関数に適切な初期値を設定し、ベルマン方程式を繰り返し適用することで、価値関数の精度を向上させます。

動的計画法による最適行動の獲得には、主に2つのアプローチがあります。

### 価値反復法 (Value Iteration)

エージェントは各状態の価値を算出し、その価値が最も高くなる状態に遷移するように行動を決定します。価値反復法は、最適な価値関数 $V^*(s)$ を直接求めることを目指します。

$$ V*{k+1}(s) = \max_a \left\{ \sum*{s'} T(s'|s,a) (R(s,a,s') + \gamma V_k(s')) \right\} $$
この反復計算により、$V_k(s)$ は最適な価値関数 $V^*(s)$ に収束します。最適な価値関数が得られれば、各状態での最適な行動は、その価値を最大化する行動として決定できます。

### 方策反復法 (Policy Iteration)

方策反復法は、「方策評価」と「方策改善」の2つのステップを交互に繰り返すことで、最適な方策を求めます。

1.  **方策評価 (Policy Evaluation)**: 現在の方策 $\pi$ の下での価値関数 $V_\pi(s)$ を計算します。これは、ベルマン方程式を解くことで行われます。
    $$ V*\pi(s) = \sum_a \pi(a|s) \sum*{s'} T(s'|s,a) (R(s,a,s') + \gamma V\_\pi(s')) $$
2.  **方策改善 (Policy Improvement)**: 評価された価値関数 $V_\pi(s)$ を用いて、現在の方策 $\pi$ よりも良い方策 $\pi'$ を貪欲に決定します。
    $$ \pi'(s) = \arg\max*a \left\{ \sum*{s'} T(s'|s,a) (R(s,a,s') + \gamma V\_\pi(s')) \right\} $$
    このプロセスを繰り返すことで、方策と価値関数は最適なものに収束します。

## モデルベースとモデルフリーの違い

モデルベース強化学習では、環境の遷移関数や報酬関数が既知（または推定可能）であるため、実際にエージェントを環境で動かすことなく、これらのモデル情報のみから最適な方策を計算することができます。

一方、モデルフリー強化学習では、環境のモデルが未知であるため、エージェントが実際に環境と相互作用し、その経験（状態、行動、報酬、次の状態の系列）から直接、最適な方策や価値関数を学習します。

## 参考

- 久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔泳社 (2019)
