---
title: "モデルフリー強化学習"
date: 2021-03-19T12:00:23+09:00
draft: false
tags: ["強化学習"] 
---
<!--more-->
モデルフリー強化学習は、環境のモデル（状態遷移関数 $T(s'|s,a)$ や報酬関数 $R(s,s')$）が**未知**であることを前提とし、エージェントが自ら環境と相互作用して経験を蓄積し、その経験から直接最適な方策を学習する手法です。

## 探索と活用のトレードオフ (Exploration-Exploitation Trade-off)

強化学習では、エージェントは「これまでに得た知識を最大限に活用して報酬を最大化する（**活用**）」ことと、「まだ知らない行動や状態を試して新たな知識を得る（**探索**）」ことの間でバランスを取る必要があります。これを「探索と活用のトレードオフ」と呼びます。

無限に行動できるのであれば、十分に探索した後に活用に専念すれば良いですが、多くの場合、行動回数には制限があります。このトレードオフのバランスを取る一般的な手法として、**ε-Greedy法**があります。

-   **ε-Greedy法**:
    -   確率 ε でランダムな行動を選択し（探索）、
    -   確率 1-ε で現在の知識に基づいて最も良いとされる行動を選択します（活用）。
    学習の初期段階では ε を大きくして探索を促し、学習が進むにつれて ε を小さくして活用を重視するように調整することが一般的です。

## 計画の修正：モンテカルロ法とTD法

エージェントが経験から価値関数や方策を更新する際、いつ、どのように更新を行うかによって、いくつかの手法があります。

### モンテカルロ法 (Monte Carlo Method)

モンテカルロ法は、**1エピソードが終了した後**に、そのエピソードで実際に得られた報酬の総和（収益）に基づいて価値を更新する手法です。

$$ V(s_t) \leftarrow V(s_t) + \alpha (G_t - V(s_t)) $$
ここで $G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots + \gamma^{T-t-1} r_T$ は時刻 $t$ 以降の割引報酬和です。

-   **特徴**: 実際に得られた報酬を用いるため、バイアスがありません。しかし、1エピソードが終了するまで更新できないため、エピソードが長い問題には不向きです。

### TD法 (Temporal-Difference Learning)

TD法は、**1ステップごと**に価値を更新する手法です。現在の価値の見積もりと、1ステップ先の報酬と価値の見積もり（TDターゲット）との差（**TD誤差**）に基づいて更新を行います。

$$ V(s_t) \leftarrow V(s_t) + \alpha (r_{t+1} + \gamma V(s_{t+1}) - V(s_t)) $$
ここで $r_{t+1} + \gamma V(s_{t+1})$ がTDターゲットです。

-   **特徴**: エピソードの途中で更新できるため、モンテカルロ法よりも高速に学習が進むことがあります。しかし、将来の価値の見積もりを用いるため、ブートストラップ（自己参照）によるバイアスが生じる可能性があります。

### TD($\lambda$)法

TD($\lambda$)法は、モンテカルロ法とTD法の中間的な手法です。TD誤差を、1ステップ先だけでなく、複数ステップ先の報酬と価値の見積もり（n-step TDターゲット）を考慮して計算します。パラメータ $\lambda$ を調整することで、モンテカルロ法（$\lambda=1$）からTD法（$\lambda=0$）まで連続的に変化させることができます。

## 価値ベースと方策ベース

モデルフリー強化学習も、モデルベース強化学習と同様に、価値評価の更新に重点を置くか、方策の更新に重点を置くかで分類されます。行動価値を $Q(s,a)$ で表します。

### 価値ベース (Value-Based)

価値関数（特に行動価値関数 $Q(s,a)$）を学習し、その価値関数に基づいて行動を選択します。

-   **Q学習 (Q-Learning)**:
    $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha (r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) $$
    Q学習は、次の状態での最大行動価値を用いるため、**方策オフ型 (Off-Policy)** の手法です。つまり、学習に用いる行動選択の方策（例: ε-Greedy）と、学習によって得られる最適な方策が異なります。

### 方策ベース (Policy-Based)

方策そのもの（各状態での行動確率）を直接学習します。

-   **SARSA**:
    $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha (r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) $$
    SARSAは、次の状態 $s_{t+1}$ で実際に選択された行動 $a_{t+1}$ の行動価値を用いるため、**方策オン型 (On-Policy)** の手法です。つまり、学習に用いる行動選択の方策と、学習によって得られる方策が同じです。

### Actor-Critic法

価値ベースと方策ベースを組み合わせた手法です。

-   **Actor (アクター)**: 方策を学習し、行動を選択する部分。
-   **Critic (クリティック)**: 価値関数を学習し、アクターが選択した行動を評価する部分。

アクターとクリティックが相互に情報を更新し合いながら学習を進めます。

## 参考
-   久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔泳社 (2019)
