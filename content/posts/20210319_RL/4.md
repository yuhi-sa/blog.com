---
title: "ニューラルネットワークを適用した強化学習"
date: 2021-03-22T10:00:23+09:00
draft: false
tags: ["強化学習", "深層学習"] 
---
<!--more-->
# ニューラルネットワークを適用した強化学習

強化学習において、状態空間や行動空間が非常に大きい場合、価値関数や方策をテーブル形式で表現することが困難になります。このような場合に、ニューラルネットワーク（NN）を用いて価値関数や方策を近似する手法が用いられます。

しかし、NNを強化学習に適用する際には、学習が不安定になりやすいという課題があります。これを解決するために、様々な工夫が提案されています。

## 学習安定化のための主要な工夫

### 1. Experience Replay (経験再生)

エージェントが環境と相互作用して得た経験（状態、行動、報酬、次の状態）を**リプレイバッファ**と呼ばれるメモリに蓄積します。学習時には、このリプレイバッファからランダムに経験のミニバッチをサンプリングしてNNを更新します。

-   **効果**:
    -   経験間の相関を低減し、学習の安定性を向上させます。
    -   同じ経験を複数回利用できるため、データの利用効率が向上します。

### 2. Fixed Target Q-Network (固定ターゲットQネットワーク)

Q学習において、ターゲット値（目標となるQ値）の計算に用いるQネットワークのパラメータを、一定期間固定する手法です。

-   **課題**: 通常のQ学習では、ターゲットQ値の計算と、Qネットワークの更新に同じネットワークが使われるため、目標値が常に変動し、学習が不安定になりやすいです。
-   **効果**: ターゲットQネットワークのパラメータを固定することで、目標値が安定し、学習の収束性が向上します。一定のステップごとにターゲットQネットワークのパラメータをメインのQネットワークのパラメータで更新します。

### 3. Reward Clipping (報酬のクリッピング)

報酬のスケールが非常に大きい場合や、報酬の分布が偏っている場合に、報酬を一定の範囲（例: [-1, 1]）にクリッピングする手法です。

-   **効果**: 報酬のスケールが大きすぎることによる勾配の不安定化を防ぎ、学習を安定させます。

## Deep Q-Network (DQN) とその改良

**Deep Q-Network (DQN)** は、Q学習に深層学習（ニューラルネットワーク）と上記の安定化技術（Experience Replay, Fixed Target Q-Network）を組み合わせた画期的な手法です。Google DeepMindが開発し、Atariゲームで人間を超える性能を示しました。

DQNの発表以降、さらなる性能向上を目指して様々な改良手法が提案されています。DeepMindは、これらの改良手法のいくつかを組み合わせた「**Rainbow**」というモデルも発表しています。

### DQNの主な改良手法

1.  **Double DQN (DDQN)**
    -   **目的**: Q値の過大評価を抑制し、価値の見積もり精度を向上させます。
    -   **仕組み**: 行動選択とターゲットQ値の計算に用いるネットワークを分離します。行動選択はメインのQネットワークで行い、その行動のターゲットQ値はターゲットQネットワークで計算します。

2.  **Prioritized Experience Replay (PER)**
    -   **目的**: 学習効率を向上させます。
    -   **仕組み**: Experience Replayにおいて、単にランダムに経験をサンプリングするのではなく、TD誤差が大きい（つまり、学習効果が高い）経験を優先的にサンプリングします。

3.  **Dueling Network Architectures (Dueling DQN)**
    -   **目的**: 価値の見積もり精度を向上させます。
    -   **仕組み**: Q値を「状態価値 (State-Value)」と「アドバンテージ (Advantage)」に分解して計算するネットワーク構造を採用します。これにより、各行動の価値をより正確に評価できるようになります。

4.  **Multi-step Learning (N-step TD)**
    -   **目的**: 価値の見積もり精度を向上させます。
    -   **仕組み**: Q学習やSARSAのような1ステップTD更新ではなく、nステップ先の報酬と価値の見積もりを用いて更新を行います。モンテカルロ法とTD法の中間的な性質を持ちます。

5.  **Distributional RL (C51, QR-DQNなど)**
    -   **目的**: 価値の見積もり精度を向上させます。
    -   **仕組み**: Q値を単一の値として推定するのではなく、報酬の分布そのものを学習します。これにより、より豊かな情報に基づいて行動を決定できるようになります。

6.  **Noisy Nets**
    -   **目的**: 探索効率を向上させます。
    -   **仕組み**: ε-Greedy法のように手動で ε を設定する代わりに、ネットワークの重みにノイズを加えることで、エージェント自身が探索の度合いを学習できるようにします。これにより、探索のバランス調整が自動化されます。

## 参考
-   久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔栄社 (2019)