---
title: "変分オートエンコーダ (VAE)"
date: 2021-03-18T12:00:23+09:00
draft: false
tags: ["ベイズ統計", "機械学習", "深層学習"]
description: "変分オートエンコーダ（VAE）の仕組みを解説。エンコーダ・デコーダの構造、ELBOの導出、再パラメータ化トリックまで詳しく説明します。"
---

<!--more-->

変分オートエンコーダ (Variational AutoEncoder, VAE) は、生成モデルの一種であり、データの潜在的な構造を学習し、新しいデータを生成することを目的としています。EMアルゴリズムが対数尤度下界を最大化するのと同様に、VAEも変分下界（ELBO）を最大化することで学習を行います。

VAEの大きな特徴は、**確率的なエンコーダ（認識モデル）**と**確率的なデコーダ（生成モデル）**を持つ点です。これにより、潜在空間が滑らかになり、意味のあるデータ生成が可能になります。

- **認識モデル（エンコーダ）**: 入力データ $x$ から、潜在変数 $z$ の確率分布 $q_\phi(z|x)$ を推定します。パラメータ $\phi$ を持ちます。
- **生成モデル（デコーダ）**: 潜在変数 $z$ から、データ $x$ の確率分布 $p_\theta(x|z)$ を生成します。パラメータ $\theta$ を持ちます。

潜在変数 $z$ は、入力データ $x$ の持つ情報をより低次元で抽象的な「潜在表現」または「潜在コード」として表現していると解釈できます。

## オートエンコーダ (AutoEncoder)

オートエンコーダは、入力データを圧縮・符号化し、それを再構築（復号化）することで、入力データと同じ内容を出力するように学習するニューラルネットワークです。中間層（潜在空間）は、入力データの重要な特徴を捉えた圧縮表現（符号）となります。

VAEは、このオートエンコーダの枠組みに変分推論の概念を導入したものです。

## VAEにおける変分下界 (ELBO)

VAEの学習は、以下の変分下界（ELBO）を最大化することによって行われます。

$$ \mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - KL(q\_\phi(z|x) || p(z)) $$

この式は、以下の2つの項から構成されます。

1.  **再構成誤差（第一項）**: $\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$
    エンコーダが生成した潜在変数 $z$ を使って、デコーダが元の入力 $x$ をどれだけ正確に再構成できるかを表します。この項を最大化することは、再構成誤差を最小化することに相当します。

2.  **正則化項（第二項）**: $KL(q_\phi(z|x) || p(z))$
    エンコーダが推定した潜在変数 $z$ の分布 $q_\phi(z|x)$ が、事前に定めた**潜在変数の事前分布** $p(z)$ にどれだけ近いかを表します。この項を最小化することは、潜在空間を滑らかにし、意味のあるデータ生成を可能にする役割があります。

### 認識モデル $q_\phi(z|x)$

認識モデルは、入力 $x$ から潜在変数 $z$ の分布を推定します。一般的に、ニューラルネットワークを用いて、潜在変数 $z$ が従う多変量正規分布の平均 $\mu(x)$ と分散 $\sigma^2(x)$（または対数分散 $\log \sigma^2(x)$）を出力します。

$$ q*\phi(z|x) = \mathcal{N}(z | \mu*\phi(x), \text{diag}(\sigma^2\_\phi(x))) $$

ここで、$\mu_\phi(x)$ と $\sigma^2_\phi(x)$ は、入力 $x$ を受け取るニューラルネットワークの出力です。

### 生成モデル $p_\theta(x|z)$

生成モデルは、潜在変数 $z$ からデータ $x$ の分布を生成します。どのような確率分布を用いるかは、データ $x$ の種類に依存します。

- **二値データ（例: 白黒画像）**: ベルヌーイ分布（またはカテゴリカル分布）
- **連続値データ（例: グレースケール画像）**: ガウス分布

例えば、連続値データの場合、分散を1と仮定した多変量正規分布を用いることがあります。

$$ p*\theta(x|z) = \mathcal{N}(x | \nu*\theta(z), I) $$

ここで、$\nu_\theta(z)$ は、潜在変数 $z$ を受け取るニューラルネットワークの出力です。

### 潜在変数の事前分布 $p(z)$

潜在変数の事前分布は、通常、標準正規分布（平均0、分散1）の積として定義されます。

$$ p(z) = \prod\_{j=1}^k \mathcal{N}(z_j | 0, 1) $$

この事前分布は、学習中に固定され、パラメータ $\theta$ や $\phi$ には依存しません。

## 勾配降下法と再パラメータ化トリック

VAEの学習は、変分下界 $\mathcal{L}(\theta, \phi)$ を最大化するために、勾配降下法（Adamなどの最適化アルゴリズム）を用いてパラメータ $\theta$ と $\phi$ を更新します。

再構成誤差項の勾配は比較的容易に計算できますが、正則化項の $KL(q_\phi(z|x) || p(z))$ の勾配を直接計算することは困難です。
これは、期待値の計算が $\phi$ に依存する $q_\phi(z|x)$ 上で行われるためです。

この問題を解決するために、**再パラメータ化トリック (Reparameterization Trick)** が用いられます。
これは、潜在変数 $z$ を、パラメータ $\phi$ に依存しない確率変数 $\epsilon$ と、パラメータ $\phi$ に依存する決定的な関数 $g(\epsilon, x, \phi)$ を用いて表現する手法です。

例えば、ガウス分布の場合、$z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon$ と表現できます（ここで $\epsilon \sim \mathcal{N}(0, I)$）。
これにより、期待値の計算が $\phi$ に依存しない $\epsilon$ 上で行われるようになり、通常の勾配降下法を適用できるようになります。

## 参考

- 手塚 太郎, 『しくみがわかるベイズ統計と機械学習』, 講談社 (2017)
