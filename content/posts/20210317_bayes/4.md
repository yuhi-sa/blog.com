---
title: "正規分布における最尤推定"
date: 2021-03-16T13:00:23+09:00
draft: false
tags: ["統計学", "機械学習"]
keywords: ["統計学", "機械学習"]
description: "正規分布を例に最尤推定の考え方を解説。尤度関数・対数尤度関数の定義から、平均と分散の最尤推定量の導出過程を詳しく説明します。"
---

<!--more-->

最尤推定（Maximum Likelihood Estimation, MLE）は、統計モデルのパラメータを推定する一般的な手法の一つです。20世紀初頭に統計学者ロナルド・フィッシャーによって体系化されました。

### 最尤推定の考え方

最尤推定では、**尤度関数**を最大化するパラメータを推定します。尤度関数とは、「**与えられたデータが、特定のパラメータを持つモデルから生成される確率（または確率密度）**」と解釈できる関数です。

観測データ $x = \{x_1, x_2, \dots, x_n\}$ があり、これがパラメータ $\theta$ を持つ確率分布 $p(x|\theta)$ に従うと仮定します。各観測が独立同分布（i.i.d.）である場合、尤度関数 $L(\theta|x)$ は、各観測値の確率の積として定義されます。

$$ L(\theta|x) = p(x|\theta) = \prod\_{i=1}^n p(x_i|\theta) $$

最尤推定は、この尤度関数 $L(\theta|x)$ を最大化するパラメータ $\hat{\theta}_{ML}$ を求めることです。

### 対数尤度関数

尤度関数は積の形をしているため、計算が複雑になることがあります。また、積の微分は扱いにくいです。そこで、単調増加関数である対数をとった**対数尤度関数** $\log L(\theta|x)$ を最大化することが一般的です。対数をとることで積が和になり、微分が容易になります。

$$ \log L(\theta|x) = \sum\_{i=1}^n \log p(x_i|\theta) $$

対数関数は単調増加であるため、尤度関数を最大化することと対数尤度関数を最大化することは等価です。

### 正規分布における最尤推定

ここでは、観測データ $x = \{x_1, x_2, \dots, x_n\}$ が正規分布 $\mathcal{N}(x | \mu, \sigma^2)$ に従うと仮定し、そのパラメータである平均 $\mu$ と分散 $\sigma^2$ を最尤推定で求めてみます。

正規分布の確率密度関数は以下の通りです。
$$ p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) $$

この対数尤度関数は、以下のようになります。
$$ \log L(\mu, \sigma^2 | x) = -\frac{1}{2\sigma^2} \sum\_{i=1}^n (x_i - \mu)^2 - \frac{n}{2} \log(2\pi\sigma^2) $$

#### 平均 $\mu$ の最尤推定

対数尤度関数を $\mu$ で偏微分し、0と置きます。

$$ \frac{\partial \log L}{\partial \mu} = -\frac{1}{2\sigma^2} \sum*{i=1}^n 2(x_i - \mu)(-1) = \frac{1}{\sigma^2} \sum*{i=1}^n (x_i - \mu) $$

これを0と置くと、
$$ \sum*{i=1}^n (x_i - \mu) = 0 \implies \sum*{i=1}^n x*i - n\mu = 0 \implies \hat{\mu}*{ML} = \frac{1}{n} \sum\_{i=1}^n x_i $$

したがって、正規分布の平均の最尤推定量は、**標本平均**と一致します。これが、算術平均が統計学で広く用いられる理由の一つです。

#### 分散 $\sigma^2$ の最尤推定

対数尤度関数を $\sigma^2$ で偏微分し、0と置きます。

$$ \frac{\partial \log L}{\partial \sigma^2} = -\frac{1}{2} \sum*{i=1}^n (x_i - \mu)^2 \left(-\frac{1}{(\sigma^2)^2}\right) - \frac{n}{2} \frac{1}{\sigma^2} $$
$$ = \frac{1}{2(\sigma^2)^2} \sum*{i=1}^n (x_i - \mu)^2 - \frac{n}{2\sigma^2} $$

これを0と置くと、
$$ \frac{1}{(\sigma^2)^2} \sum*{i=1}^n (x_i - \mu)^2 = \frac{n}{\sigma^2} \implies \hat{\sigma}^2*{ML} = \frac{1}{n} \sum\_{i=1}^n (x_i - \mu)^2 $$

ここで $\mu$ には、先ほど求めた最尤推定量 $\hat{\mu}_{ML}$ を代入します。

$$ \hat{\sigma}^2*{ML} = \frac{1}{n} \sum*{i=1}^n (x*i - \hat{\mu}*{ML})^2 $$

これは、**標本分散**の定義と一致します。ただし、不偏分散（$n-1$ で割る）とは異なる点に注意が必要です。最尤推定量としての標本分散は、真の分散をわずかに過小評価する傾向があります。

## 参考

- 手塚 太郎, 『しくみがわかるベイズ統計と機械学習』, 講談社 (2017)
