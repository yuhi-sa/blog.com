---
title: "強化学習によるROBOTIS OP3の歩行獲得：ROSパッケージのコード解説"
date: 2021-11-02T10:17:23+09:00
draft: false
tags: ["強化学習", "ROS", "ロボット"] 
---
<!--more-->
# 強化学習によるROBOTIS OP3の歩行獲得：ROSパッケージのコード解説

## はじめに

本記事では、ROBOTIS OP3ヒューマノイドロボットがGazeboシミュレーション環境内で強化学習を用いて歩行を獲得するROS（Robot Operating System）パッケージのコードについて解説します。

-   関連リポジトリ: [op3_walk](https://github.com/yuhi-sa/op3_walk)

## 結果の動画

学習によって歩行を獲得したOP3の動作は、以下の動画で確認できます。

-   [op3_controller_demo](https://github.com/yuhi-sa/op3_walk/blob/main/docs/op3_controller_demo.mp4)

## 手法の説明

本プロジェクトでは、**深層Qネットワーク (Deep Q-Network, DQN)** を使用しています。DQNは、Q学習に深層学習を組み合わせた手法で、行動価値関数をニューラルネットワークで近似します。

行動価値関数 $Q(s_t, a_t)$ は、3層のニューラルネットワークとして定義され、次のQ学習の更新式に基づいて学習されます。

$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \eta (R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) $$

ここで、$\eta$ は学習率、$R_{t+1}$ は即時報酬、$\gamma$ は割引率です。

ニューラルネットワークの更新には、次の損失関数 $L$ を用いて誤差逆伝播を行います。

$$ L = \mathbb{E}[(R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t))^2] $$

## プログラムの構成

本ROSパッケージは、主に次のPythonスクリプトで構成されています。

### 1. `function.py` および `motion.py`

-   [`function.py`](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/function.py): 強化学習エージェントの基本的な定義が含まれています。
    -   `Agent` クラス: ニューラルネットワークを定義する `Brain` クラスを内包しています。
    -   `ReplayMemory` クラス: エージェントが環境から得た経験（行動と状態）を蓄積し、`Brain` がこのメモリからサンプリングして損失計算とニューラルネットワークの更新を行います。
    -   行動は離散化されており、`epsilon-greedy` 法に基づいて選択されます。
-   [`motion.py`](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/motion.py): ロボットの具体的な離散行動（例: 各関節の目標角度）が定義されています。

これらのスクリプトは、以下の書籍のコードを参考にしています。
-   [Deep-Reinforcement-Learning-Book](https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book)

### 2. `learning.py`

-   [`learning.py`](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/learning.py): `function.py` で定義された `Agent` クラスを継承し、ROSノードとして動作します。
    -   `controller.py` からロボットの状態をROSトピックとして購読（subscribe）します。
    -   購読した状態を入力として、エージェントが行動を計算し、その行動をROSトピックとして公開（publish）します。
    -   ニューラルネットワークの定義にはPyTorchを使用しているため、Python 3系で実行する必要があります。

### 3. `controller.py`

-   [`controller.py`](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/controller.py): `learning.py` から公開された行動をROSトピックとして購読し、実際にROBOTIS OP3をGazeboシミュレーション内で動かします。
    -   ロボットの現在の状態（関節角度、重心位置など）をROSトピックとして公開します。
    -   このスクリプトは、OP3のROSパッケージの依存関係上、Python 2系で実行する必要があります。

## 学習曲線

学習の進行に伴う歩行距離の変化を示したグラフです。世代が進むにつれて歩行距離が伸び、エージェントが効率的な歩行を学習していることがわかります。

![歩行距離](https://github.com/yuhi-sa/op3_walk/blob/main/docs/learning.png?raw=true)