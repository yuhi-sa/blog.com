---
title: "変分ベイズ法"
date: 2021-03-18T10:00:23+09:00
draft: false
tags: ["ベイズ統計", "機械学習"] 
---
<!--more-->
# 変分ベイズ法

変分ベイズ法は、複雑な事後分布を直接計算することが困難な場合に、より扱いやすい近似分布を用いて推論を行う手法です。EMアルゴリズムと同様に「変分下界」の概念を利用しますが、EMアルゴリズムがパラメータの点推定を行うのに対し、変分ベイズ法はパラメータの**分布**を推定する点で異なります。

-   **変分**: 関数を引数とする関数（汎関数）の微分を指します。
-   **汎関数**: 関数を入力として受け取り、スカラー値を出力する関数です。

## 変分ベイズにおける変分下界

### EMアルゴリズムにおける変分下界

EMアルゴリズムでは、観測データ $x$ の対数尤度 $\log p(x|\theta)$ の下界として、以下の変分下界 $\mathcal{L}(\theta, \hat{\theta})$ を考えました。

$$ \log p(x|\theta) = \mathcal{L}(\theta, \hat{\theta}) + KL(p(z|x,\hat{\theta})||p(z|x,\theta)) $$

ここで、$\mathcal{L}(\theta, \hat{\theta})$ はQ関数とエントロピーの和として表現されます。

### 変分ベイズにおける変分下界

変分ベイズ法では、パラメータ $\theta$ と隠れ変数 $z$ をまとめて潜在変数 $w = (\theta, z)$ と考えます。そして、真の事後分布 $p(w|x)$ を近似する分布 $q(w)$ を導入します。変分ベイズの目的は、この近似分布 $q(w)$ を真の事後分布に近づけることで、観測データ $x$ の対数周辺尤度 $\log p(x)$ の下界を最大化することです。

$$ \log p(x) = \mathcal{L}(q) + KL(q(w)||p(w|x)) $$

ここで、$\mathcal{L}(q)$ が変分ベイズにおける変分下界です。

$$ \mathcal{L}(q) = \int q(w) \log \frac{p(x,w)}{q(w)} dw $$

この変分下界は、期待値の形で以下のように表現できます。

$$ \mathcal{L}(q) = \mathbb{E}_{q(w)}[\log p(x,w)] - \mathbb{E}_{q(w)}[\log q(w)] $$

変分下界 $\mathcal{L}(q)$ を最大化することは、真の事後分布 $p(w|x)$ と近似分布 $q(w)$ の間のKLダイバージェンス $KL(q(w)||p(w|x))$ を最小化することと等価です。なぜなら、$\log p(x)$ は $q(w)$ に依存しない定数であり、KLダイバージェンスは常に非負だからです。

変分ベイズ法では、通常、近似分布 $q(w)$ が潜在変数の各要素の積で表されると仮定します（**平均場近似**）。

$$ q(w) = \prod_i q_i(w_i) $$

この仮定のもとで変分下界を最大化すると、各 $q_i(w_i)$ の更新式が導出され、これを繰り返すことで最適な近似分布を見つけます。

## 参考
-   手塚 太郎, 『しくみがわかるベイズ統計と機械学習』, 講談社 (2017)